{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 6963122,
          "sourceType": "datasetVersion",
          "datasetId": 4000045
        }
      ],
      "dockerImageVersionId": 30579,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Glioma Grade Classification Project\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The goal of this project is to develop a machine learning model capable of accurately detecting the grade of glioma in individuals. Gliomas are a type of brain tumor, and accurately identifying the grade is crucial for determining the appropriate treatment for patients. The target variable in this project is the glioma grade, with two classes: \"LGG\" (Low-Grade Glioma) and \"GBM\" (Glioblastoma Multiforme).\n",
        "\n",
        "In order to carry this project, we are going to use dataset from 'Hierarchical Voting-Based Feature Selection and Ensemble Learning Model Scheme for Glioma Grading with Clinical and Molecular Characteristics By E. Tasci, Y. Zhuge, Harpreet Kaur, K. Camphausen, A. Krauze. 2022'. The author managed to get 87.606% using 20.5 (mean) number of features. By the end of this project, we found out that our KNN model managed to get to 87.8% accuracy by only using 5 features.\n",
        "\n",
        "## Project Objective\n",
        "\n",
        "The primary objective is to leverage clinical and molecular data to train a model that can predict the glioma grade. Molecular analysis, while informative, can be expensive. Therefore, the focus is on identifying the most relevant features among the 20 most recently mutated genes, while also addressing 3 features regarding clinical/demography data of the patients. These features will be used to build a predictive model that can assist in glioma grade classification.\n",
        "\n",
        "## Dataset Features\n",
        "\n",
        "### Demographic Information\n",
        "\n",
        "- **Grade (Target):** Glioma grade class information. (*LGG*, *GBM*-->*0*,*1*)\n",
        "- **Gender:** Gender information. (*MALE*, *FEMALE*)\n",
        "- **Age at Diagnosis:** Age at diagnosis measured in years.\n",
        "- **Race:** Race information.\n",
        "\n",
        "### Molecular Mutation Status\n",
        "\n",
        "- **IDH1:** Isocitrate dehydrogenase 1 mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **TP53:** Tumor protein p53 mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **ATRX:** ATRX chromatin remodeler mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **PTEN:** Phosphatase and tensin homolog mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **EGFR:** Epidermal growth factor receptor mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **CIC:** Capicua transcriptional repressor mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **MUC16:** Mucin 16 mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **PIK3CA:** Phosphatidylinositol-4,5-bisphosphate 3-kinase catalytic subunit alpha mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **NF1:** Neurofibromin 1 mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **PIK3R1:** Phosphoinositide-3-kinase regulatory subunit 1 mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **FUBP1:** Far upstream element binding protein 1 mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **RB1:** RB transcriptional corepressor 1 mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **NOTCH1:** Notch receptor 1 mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **BCOR:** BCL6 corepressor mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **CSMD3:** CUB and Sushi multiple domains 3 mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **SMARCA4:** SWI/SNF related, matrix associated, actin-dependent regulator of chromatin, subfamily a, member 4 mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **GRIN2A:** Glutamate ionotropic receptor NMDA type subunit 2A mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **IDH2:** Isocitrate dehydrogenase (NADP(+)) 2 mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **FAT4:** FAT atypical cadherin 4 mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "- **PDGFRA:** Platelet-derived growth factor receptor alpha mutation status (*NOT_MUTATED*, *MUTATED*).\n",
        "\n",
        "### Additional Information\n",
        "\n",
        "- **Project:** Corresponding TCGA-LGG or TCGA-GBM project names.\n",
        "- **Case_ID:** Related project Case_ID information.\n",
        "- **Primary_Diagnosis:** Type of primary diagnosis information.\n",
        "\n",
        "## Importance of Glioma Grade Detection\n",
        "\n",
        "Accurate glioma grade detection is vital for guiding healthcare professionals in determining the most appropriate treatment strategies for patients. By analyzing clinical and molecular features, we aim to build a robust predictive model that can contribute to the accurate diagnosis of glioma, ultimately improving patient outcomes.\n"
      ],
      "metadata": {
        "id": "b5sHCAaI3lIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Importing and Preprocessing"
      ],
      "metadata": {
        "id": "uzs9fwG03lIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before Split"
      ],
      "metadata": {
        "id": "emFCTyvr3lII"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we import numpy and pandas because this is rudimentary for our analysis. **Note: Please install numpy==1.23.5, otherwise there will be one code cell that cannot be executed.**"
      ],
      "metadata": {
        "id": "UE8-tWj23lII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-11-17T09:37:22.865395Z",
          "iopub.execute_input": "2023-11-17T09:37:22.865782Z",
          "iopub.status.idle": "2023-11-17T09:37:22.880755Z",
          "shell.execute_reply.started": "2023-11-17T09:37:22.86575Z",
          "shell.execute_reply": "2023-11-17T09:37:22.879766Z"
        },
        "trusted": true,
        "id": "kU3d6kmt3lII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then we import the data, and drop duplicates as it may cause bias in the feature weight."
      ],
      "metadata": {
        "id": "CawJEUWo3lIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing dataset\n",
        "df = pd.read_csv('/kaggle/input/glioma-grade-classification/TCGA_GBM_LGG_Mutations_all.csv')\n",
        "#Dropping duplicates\n",
        "df=df.drop_duplicates()\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:37:22.882535Z",
          "iopub.execute_input": "2023-11-17T09:37:22.882841Z",
          "iopub.status.idle": "2023-11-17T09:37:22.918045Z",
          "shell.execute_reply.started": "2023-11-17T09:37:22.882814Z",
          "shell.execute_reply": "2023-11-17T09:37:22.917144Z"
        },
        "trusted": true,
        "id": "f47M5R2V3lIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then we will drop three columns: Project, Case_ID, and Primary_Diagnosis. Project and Case_ID are arbitrary identifier that is not necessary for our analysis while Primary_Diagnosis seems to correlate a lot with Glioma's grade which only kill the purpose of our analysis if we include it."
      ],
      "metadata": {
        "id": "Hg4av8BL3lIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping columns: Project, Case_ID, Primary_Diagnosis as they might not help with our modelling\n",
        "df=df.drop(['Project','Case_ID','Primary_Diagnosis'],axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:37:22.919311Z",
          "iopub.execute_input": "2023-11-17T09:37:22.919596Z",
          "iopub.status.idle": "2023-11-17T09:37:22.943248Z",
          "shell.execute_reply.started": "2023-11-17T09:37:22.919571Z",
          "shell.execute_reply": "2023-11-17T09:37:22.94215Z"
        },
        "trusted": true,
        "id": "IA_BtCsd3lIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then we check for any NaN value using .info() attribute."
      ],
      "metadata": {
        "id": "EKj237FF3lIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the overall information regarding our dataframe\n",
        "df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:37:22.94463Z",
          "iopub.execute_input": "2023-11-17T09:37:22.945042Z",
          "iopub.status.idle": "2023-11-17T09:37:22.957546Z",
          "shell.execute_reply.started": "2023-11-17T09:37:22.945005Z",
          "shell.execute_reply": "2023-11-17T09:37:22.956627Z"
        },
        "trusted": true,
        "id": "ech9pIVS3lIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Double check with .value_counts(). As you can see below, there actually happened to be some problematic values such as '--' and 'not_reported'."
      ],
      "metadata": {
        "id": "85IFGOEk3lIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Making sure there are no problematic values other than NaN values\n",
        "for col in df.columns:\n",
        "    print(df[col].value_counts())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:37:22.959813Z",
          "iopub.execute_input": "2023-11-17T09:37:22.960116Z",
          "iopub.status.idle": "2023-11-17T09:37:22.984849Z",
          "shell.execute_reply.started": "2023-11-17T09:37:22.960089Z",
          "shell.execute_reply": "2023-11-17T09:37:22.983904Z"
        },
        "trusted": true,
        "id": "DhugbAoi3lIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore for now we will replace '--' and 'not_reported' by NaN. This will it easier for us to impute them later."
      ],
      "metadata": {
        "id": "JdbhoFft3lIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace '--' and 'not reported' with NaN and then we will impute them later\n",
        "df.replace(['--','not reported'], np.nan, inplace=True)\n",
        "print(f'features to be imputed:{df.columns[df.isnull().any()]}')\n",
        "\n",
        "# Display the DataFrame after dropping rows\n",
        "df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:37:22.986078Z",
          "iopub.execute_input": "2023-11-17T09:37:22.98635Z",
          "iopub.status.idle": "2023-11-17T09:37:23.004051Z",
          "shell.execute_reply.started": "2023-11-17T09:37:22.986326Z",
          "shell.execute_reply": "2023-11-17T09:37:23.003156Z"
        },
        "trusted": true,
        "id": "euNdgK793lIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then we convert the format of Age_at_diagnosis. Let's say if a patient's age is 30 years 182 days, we will convert it into 30.5 years."
      ],
      "metadata": {
        "id": "KQLgFmsJ3lIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert age from 'Age_at_diagnosis' column into float and check for any problematic value in it\n",
        "\n",
        "# Extract years and days\n",
        "years = df['Age_at_diagnosis'].str.extract(r'(\\d+) years').astype(float)\n",
        "days = df['Age_at_diagnosis'].str.extract(r'(\\d+) days').astype(float)\n",
        "# Fill NaN values with 0\n",
        "years = years.fillna(0)\n",
        "days = days.fillna(0)\n",
        "# Calculate the age in years with fractions\n",
        "df['Age_at_diagnosis'] = years + (days / 365)\n",
        "print(df['Age_at_diagnosis'].value_counts().sort_index())\n",
        "print(df.info())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:37:23.005331Z",
          "iopub.execute_input": "2023-11-17T09:37:23.005595Z",
          "iopub.status.idle": "2023-11-17T09:37:23.026962Z",
          "shell.execute_reply.started": "2023-11-17T09:37:23.005571Z",
          "shell.execute_reply": "2023-11-17T09:37:23.025572Z"
        },
        "trusted": true,
        "id": "EwXmEcUg3lIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then we are going to split our dataframe into training data and testing data. But before that, we should check the ratio of our target to decide whether we are going to stratify or not. Stratify is a process where we keep the ratio of our target so a 50:50 ratio in dataframe will be splitted into 50:50 ratio in training data and 50:50 ratio in test data also."
      ],
      "metadata": {
        "id": "uwXfaNZ13lIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking out the ratio of target values to find out whether we need to stratify or not later\n",
        "\n",
        "print(df['Grade'].value_counts(normalize=True))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:37:23.028477Z",
          "iopub.execute_input": "2023-11-17T09:37:23.028802Z",
          "iopub.status.idle": "2023-11-17T09:37:23.035397Z",
          "shell.execute_reply.started": "2023-11-17T09:37:23.028775Z",
          "shell.execute_reply": "2023-11-17T09:37:23.034394Z"
        },
        "trusted": true,
        "id": "PCYe5OIQ3lIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since our target has different ratio, we are going to stratify them during splitting below."
      ],
      "metadata": {
        "id": "mr0nY-2V3lIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#stratifying/splitting dataframe into train and test dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting to avoid data leakage (train : test = 80% : 20%)\n",
        "train , test = train_test_split(df,train_size=0.8, random_state=42,stratify=df.Grade) #stratify the target column"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:37:23.0367Z",
          "iopub.execute_input": "2023-11-17T09:37:23.037005Z",
          "iopub.status.idle": "2023-11-17T09:37:23.045919Z",
          "shell.execute_reply.started": "2023-11-17T09:37:23.036972Z",
          "shell.execute_reply": "2023-11-17T09:37:23.0451Z"
        },
        "trusted": true,
        "id": "vouTRhCR3lIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After Split"
      ],
      "metadata": {
        "id": "dUtvamIW3lIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have splitted our data, we can polish our train data by imputing NaN values, scaling, dropping unimportant features, and then apply the same rule to test data using the same procedure that we have applied to our training data."
      ],
      "metadata": {
        "id": "jCbhcFUn3lIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoding and Imputation"
      ],
      "metadata": {
        "id": "wJqEZQd83lIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we encode target using simple binary encoding, imputing NaN values, and the most important ones is encoding our features afterward. We will choose KFold Encoding because it will help us so much in addressing proper label that is not arbitrary (unlike label encoding/one-hot-encoding) while preventing data leakage from our target to our features.\n",
        "\n",
        "After encoding target and imputing features, we will create the KFOld custom function as shown below."
      ],
      "metadata": {
        "id": "U6nTaXgv3lIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Label our target, impute some features that have NaN values, and perform encoding for our categorical features\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Step 1: Label our target so we can do K fold target encoding\n",
        "train['Grade'] = train['Grade'].replace({'GBM': 1, 'LGG': 0})\n",
        "test['Grade'] = test['Grade'].replace({'GBM': 1, 'LGG': 0})\n",
        "\n",
        "# Step 2: Perform imputation to both training and test sets\n",
        "imputer= SimpleImputer(strategy='most_frequent')\n",
        "train[train.columns[train.isnull().any()]]=pd.DataFrame(imputer.fit_transform(train[train.columns[train.isnull().any()]]),columns=train.columns[train.isnull().any()],index=train.index)\n",
        "test[test.columns[test.isnull().any()]]=pd.DataFrame(imputer.transform(test[train.columns[test.isnull().any()]]),columns=test.columns[test.isnull().any()],index=test.index)\n",
        "\n",
        "# Impute also 0 values in Age_at_diagnosis into its median\n",
        "median_imputation=train[train['Age_at_diagnosis']!=0]['Age_at_diagnosis'].median()\n",
        "train['Age_at_diagnosis'] = train['Age_at_diagnosis'].apply(lambda x: median_imputation if x==0 else x)\n",
        "test['Age_at_diagnosis'] = test['Age_at_diagnosis'].apply(lambda x: median_imputation if x==0 else x)\n",
        "\n",
        "# Step 3: Apply K fold target encoding to both training and test sets\n",
        "\n",
        "# Define encoder function for training dataset\n",
        "from sklearn import base\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "class KFoldTargetEncoderTrain(base.BaseEstimator,\n",
        "                               base.TransformerMixin):\n",
        "    def __init__(self,colnames,targetName,\n",
        "                  n_fold=5, verbosity=True,\n",
        "                  discardOriginal_col=False):\n",
        "        self.colnames = colnames\n",
        "        self.targetName = targetName\n",
        "        self.n_fold = n_fold\n",
        "        self.verbosity = verbosity\n",
        "        self.discardOriginal_col = discardOriginal_col\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self,X):\n",
        "        mean_of_target = X[self.targetName].mean()\n",
        "        kf = KFold(n_splits = self.n_fold,\n",
        "                   shuffle = False, random_state=None)\n",
        "        col_mean_name = self.colnames + '_' + 'Kfold'\n",
        "        X[col_mean_name] = np.nan\n",
        "        for tr_ind, val_ind in kf.split(X):\n",
        "            X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind]\n",
        "            X.loc[X.index[val_ind], col_mean_name] = X_val[self.colnames].map(X_tr.groupby(self.colnames)\n",
        "                                     [self.targetName].mean())\n",
        "            X[col_mean_name].fillna(mean_of_target, inplace = True)\n",
        "        if self.verbosity:\n",
        "            encoded_feature = X[col_mean_name].values\n",
        "        if self.discardOriginal_col:\n",
        "            X = X.drop(self.targetName, axis=1)\n",
        "        return X\n",
        "\n",
        "# Define encoder function for test dataset\n",
        "class KFoldTargetEncoderTest(base.BaseEstimator, base.TransformerMixin):\n",
        "\n",
        "    def __init__(self, train, colNames, encodedName):\n",
        "        self.train = train\n",
        "        self.colNames = colNames\n",
        "        self.encodedName = encodedName\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        mean = self.train[[self.colNames, self.encodedName]].groupby(self.colNames).mean().reset_index()\n",
        "\n",
        "        dd = {}\n",
        "        for index, row in mean.iterrows():\n",
        "            dd[row[self.colNames]] = row[self.encodedName]\n",
        "\n",
        "        encoded_column = X[self.colNames].map(dd)\n",
        "        X[self.encodedName] = encoded_column\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:37:23.051217Z",
          "iopub.execute_input": "2023-11-17T09:37:23.05149Z",
          "iopub.status.idle": "2023-11-17T09:37:23.086625Z",
          "shell.execute_reply.started": "2023-11-17T09:37:23.051466Z",
          "shell.execute_reply": "2023-11-17T09:37:23.085783Z"
        },
        "trusted": true,
        "id": "XM2HvpM_3lIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then we will run the function, complete the KFold encoding, and check how our dataframe has turned out."
      ],
      "metadata": {
        "id": "RHTkTPmj3lIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in train.columns:\n",
        "    if i not in ['Grade','Age_at_diagnosis']:\n",
        "        targetc = KFoldTargetEncoderTrain(i,'Grade',n_fold=5)\n",
        "        new_train = targetc.fit_transform(train)\n",
        "        test_targetc = KFoldTargetEncoderTest(new_train,\n",
        "                                              i,\n",
        "                                              i+'_Kfold')\n",
        "        new_test = test_targetc.fit_transform(test)\n",
        "\n",
        "# Filter columns in the final train and test DataFrames\n",
        "train = new_train[['Grade', 'Age_at_diagnosis'] + [col for col in new_train.columns if '_Kfold' in col]]\n",
        "test = new_test[['Grade', 'Age_at_diagnosis'] + [col for col in new_test.columns if '_Kfold' in col]]\n",
        "print(train.head())\n",
        "print(test.head())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:37:23.088674Z",
          "iopub.execute_input": "2023-11-17T09:37:23.08899Z",
          "iopub.status.idle": "2023-11-17T09:37:23.500623Z",
          "shell.execute_reply.started": "2023-11-17T09:37:23.088964Z",
          "shell.execute_reply": "2023-11-17T09:37:23.499547Z"
        },
        "trusted": true,
        "id": "pL4wjr-y3lIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Min-Max Scaling"
      ],
      "metadata": {
        "id": "RmXxWEA_3lIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will perform Min-Max scaling to our one and only numerical data: Age_at_diagnosis. This is important to reduce the risk of Age_at_diagnosis having too much influence more than it should has to the model's prediction."
      ],
      "metadata": {
        "id": "W26un5Hr3lIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform feature scaling for our one and only numerical feature\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#Step 1: Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "#Step 2 Drop the original 'Age_at_diagnosis' column, create new series for the scaled ones, and concatenate to the train/test dataframes\n",
        "## For the training set\n",
        "train = pd.concat([train.drop(columns=['Age_at_diagnosis']),\n",
        "                   pd.DataFrame(scaler.fit_transform(train[['Age_at_diagnosis']]), columns=['Age_at_diagnosis'], index=train.index)], axis=1)\n",
        "## For the test set\n",
        "test = pd.concat([test.drop(columns=['Age_at_diagnosis']),\n",
        "                   pd.DataFrame(scaler.transform(test[['Age_at_diagnosis']]), columns=['Age_at_diagnosis'], index=test.index)], axis=1)\n",
        "print(train.head())\n",
        "print(test.head())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:37:23.502362Z",
          "iopub.execute_input": "2023-11-17T09:37:23.502966Z",
          "iopub.status.idle": "2023-11-17T09:37:23.538777Z",
          "shell.execute_reply.started": "2023-11-17T09:37:23.502926Z",
          "shell.execute_reply": "2023-11-17T09:37:23.537882Z"
        },
        "trusted": true,
        "id": "-5T-w3un3lIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dropping Unimportant Features"
      ],
      "metadata": {
        "id": "e8y4eG9j3lIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our data is ready, we can try to visualize our data to get the grasp of the characteristic of our data before dropping unnecessary features."
      ],
      "metadata": {
        "id": "BXbYdsA_3lIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Visualization Function for numerical column\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Defining plot functions\n",
        "def dist_bivariate(df):\n",
        "    num_plots = len(df.columns)\n",
        "    num_cols = 3  # Number of columns per row\n",
        "    num_rows = -(-num_plots // num_cols)  # Ceiling division to determine the number of rows\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 2 * num_rows))\n",
        "    axes = axes.flatten()\n",
        "    for i, col in enumerate(df.columns):\n",
        "        if i < num_plots:\n",
        "            sns.histplot(data=df, x=col, hue='Grade', palette='cubehelix', ax=axes[i])\n",
        "            axes[i].set_title(col)\n",
        "            axes[i].legend()\n",
        "    # Adjust layout to prevent overlap of titles and labels\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plotting the data\n",
        "dist_bivariate(train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:37:23.540341Z",
          "iopub.execute_input": "2023-11-17T09:37:23.540989Z",
          "iopub.status.idle": "2023-11-17T09:37:36.2856Z",
          "shell.execute_reply.started": "2023-11-17T09:37:23.540951Z",
          "shell.execute_reply": "2023-11-17T09:37:36.28472Z"
        },
        "trusted": true,
        "id": "bJDSKGn73lIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amazingly, we managed to find some categorical features that correlate strongly with one of the target. The most notable ones are 'IDH1' and 'PTEN' that clearly have separation between green and pink bar.\n",
        "\n",
        "Amazingly again, we found out that there are clearly separated range of age that correlates to one of the 'Grade' values. Younger people tend to be inflicted by LGG grade glioma while older ones tend to be inflicted by GBM grade glioma.\n",
        "\n",
        "These features are strong contender for our selected features."
      ],
      "metadata": {
        "id": "4toO0cPR3lIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will try to confirm it by making correlation matrix using Cramer's V method. The idea is to filter features that is highly correlated to other features to the point of redundance and doesn't add much value to our prediction.\n",
        "\n",
        "A strong correlation between variables is marked at 0.25 and above (Akoglu, H. (Year). User's guide to correlation coefficients. Marmara University School of Medicine, Department of Emergency Medicine, Istanbul, Turkey). Therefore we are going to filter out any features that have **absolute** collinearity against each other at >0.25 point."
      ],
      "metadata": {
        "id": "PriQmgAV3lIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Making cramer's matrix to spot collinearity within categorical features\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "def cramers_v(x, y):\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    chi2, _, _, _ = chi2_contingency(confusion_matrix)\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    phi2 = chi2 / n\n",
        "    r, k = confusion_matrix.shape\n",
        "    phi2corr = max(0, phi2 - ((k - 1) * (r - 1)) / (n - 1))\n",
        "    rcorr = r - ((r - 1)**2) / (n - 1)\n",
        "    kcorr = k - ((k - 1)**2) / (n - 1)\n",
        "    return np.sqrt(phi2corr / min((kcorr - 1), (rcorr - 1)))\n",
        "\n",
        "# Calculate Cramér's V for each pair of categorical variables excluding numerical columns and 'Grade'\n",
        "cramers_matrix = pd.DataFrame(index=[col for col in train.columns if col !='Age_at_diagnosis'],\n",
        "                              columns=[col for col in train.columns if col !='Age_at_diagnosis'])\n",
        "for col1 in cramers_matrix.columns:\n",
        "    for col2 in cramers_matrix.columns:\n",
        "        cramers_matrix.loc[col1, col2] = cramers_v(train[col1], train[col2])\n",
        "\n",
        "# Create a heatmap using seaborn to check for collinearity in our training dataset\n",
        "plt.figure(figsize=(20, 8))\n",
        "sns.heatmap(cramers_matrix.astype(float), annot=True, cmap='coolwarm', vmin=0, vmax=1)\n",
        "plt.title(\"Cramér's V Heatmap for Categorical Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:37:36.28795Z",
          "iopub.execute_input": "2023-11-17T09:37:36.288297Z",
          "iopub.status.idle": "2023-11-17T09:37:42.083456Z",
          "shell.execute_reply.started": "2023-11-17T09:37:36.288271Z",
          "shell.execute_reply": "2023-11-17T09:37:42.082496Z"
        },
        "trusted": true,
        "id": "yMymLFnc3lIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding all features with absolute correlation value >=0.25, and then weed out those that have lower correlation against 'Grade' value\n",
        "filtered_cramers_matrix = set()  # Use a set to automatically handle duplicates\n",
        "for col1 in cramers_matrix.columns:\n",
        "    for col2 in cramers_matrix.columns:\n",
        "        if abs(cramers_matrix.loc[col1, col2]) >= 0.25 and col1 != col2 and col1 != 'Grade' and col2 != 'Grade':\n",
        "            if abs(cramers_matrix.loc[col1, 'Grade']) > abs(cramers_matrix.loc[col2, 'Grade']):\n",
        "                filtered_cramers_matrix.add(col2)\n",
        "            else:\n",
        "                filtered_cramers_matrix.add(col1)\n",
        "\n",
        "filtered_cramers_matrix = list(filtered_cramers_matrix)  # Convert back to a list\n",
        "print(filtered_cramers_matrix)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:37:42.084859Z",
          "iopub.execute_input": "2023-11-17T09:37:42.085612Z",
          "iopub.status.idle": "2023-11-17T09:37:42.113104Z",
          "shell.execute_reply.started": "2023-11-17T09:37:42.085575Z",
          "shell.execute_reply": "2023-11-17T09:37:42.112075Z"
        },
        "trusted": true,
        "id": "c2LV7mDe3lIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But there are too many columns to drop. We shall try keeping some of the mutation features that have correlation point of 0.25 or above to the target; namely ATRK, PTEN, and CIC. Let's not forget to keep Age_at_diagnosis as our numerical feature."
      ],
      "metadata": {
        "id": "QKAFKstF3lIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the all columns with high collinearity with each others but spares ATRX, PTEN, CIC, and Age_at_diagnosis\n",
        "train = train.drop(columns=[col for col in filtered_cramers_matrix if col not in ['ATRX_Kfold','PTEN_Kfold','CIC_Kfold','Age_at_diagnosis']])\n",
        "test = test.drop(columns=[col for col in filtered_cramers_matrix if col not in ['ATRX_Kfold','PTEN_Kfold','CIC_Kfold','Age_at_diagnosis']])\n",
        "print(train.head())\n",
        "print(test.head())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:37:42.114473Z",
          "iopub.execute_input": "2023-11-17T09:37:42.114775Z",
          "iopub.status.idle": "2023-11-17T09:37:42.129098Z",
          "shell.execute_reply.started": "2023-11-17T09:37:42.114748Z",
          "shell.execute_reply": "2023-11-17T09:37:42.127997Z"
        },
        "trusted": true,
        "id": "hQ4uS3P-3lIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are done with our feature filtering. Finally we can splite our features into x_train, x_test, y_train, and y_test and start fitting them into models."
      ],
      "metadata": {
        "id": "tFYuqn3t3lIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Separate x from y in train and test dataframe\n",
        "x_train=train.copy()\n",
        "y_train=x_train.pop('Grade')\n",
        "x_test=test.copy()\n",
        "y_test=x_test.pop('Grade')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:37:42.130307Z",
          "iopub.execute_input": "2023-11-17T09:37:42.130705Z",
          "iopub.status.idle": "2023-11-17T09:37:42.136862Z",
          "shell.execute_reply.started": "2023-11-17T09:37:42.130672Z",
          "shell.execute_reply": "2023-11-17T09:37:42.136042Z"
        },
        "trusted": true,
        "id": "UbMYdQ-f3lIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Hyperparameter Tuning and Making Prediction"
      ],
      "metadata": {
        "id": "xHPJwiPT3lIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will attempt to use four classifier models as follow:\n",
        "1. Logistic Regression\n",
        "2. K-Nearest Neighbors\n",
        "3. Naive Bayes\n",
        "4. Support Vector Machine\n",
        "\n",
        "At the end, these traits will be compared:\n",
        "- Accuracy\n",
        "- Learning Curve\n",
        "- Classification Report"
      ],
      "metadata": {
        "id": "lLPqP5VQ3lIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Logistic Regression"
      ],
      "metadata": {
        "id": "zT9Z3Kn13lIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy"
      ],
      "metadata": {
        "id": "5asU8gAd3lIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from skopt import BayesSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initiate regression object, class weight refers to the count ratio of each class per total class\n",
        "logistic_regression = LogisticRegression(random_state = 42, class_weight = {0: df['Grade'].value_counts(normalize=True)[1],\n",
        "                                                                            1: df['Grade'].value_counts(normalize=True)[0]})\n",
        "\n",
        "# Apply hyperparameter tuning\n",
        "param = {\n",
        "    'tol': [0.0001, 0.001, 0.01],\n",
        "    'solver':['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
        "    'C': [0.5,1,1.5],\n",
        "    'fit_intercept':[False, True],\n",
        "}\n",
        "\n",
        "bayes_search = BayesSearchCV(estimator=logistic_regression, search_spaces=param,\n",
        "                             cv=10,n_jobs=-1,n_iter=30, scoring='roc_auc')\n",
        "bayes_search.fit(x_train, y_train)\n",
        "print(\"Parameter Terbaik:\", bayes_search.best_params_)\n",
        "\n",
        "logistic_regression = LogisticRegression(**bayes_search.best_params_,random_state = 42, class_weight = {0: df['Grade'].value_counts(normalize=True)[1],\n",
        "                                                                            1: df['Grade'].value_counts(normalize=True)[0]})\n",
        "\n",
        "# Fit model to training data\n",
        "logistic_regression.fit(x_train, y_train)\n",
        "\n",
        "# Calculating accuracy\n",
        "print(f\"Train Accuracy : {accuracy_score(y_train, logistic_regression.predict(x_train))*100} %\" )\n",
        "print(f\"Test Accuracy : {accuracy_score(y_test, logistic_regression.predict(x_test))*100} %\" )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:37:42.137812Z",
          "iopub.execute_input": "2023-11-17T09:37:42.138647Z",
          "iopub.status.idle": "2023-11-17T09:38:22.426577Z",
          "shell.execute_reply.started": "2023-11-17T09:37:42.138619Z",
          "shell.execute_reply": "2023-11-17T09:38:22.423716Z"
        },
        "trusted": true,
        "id": "LEpmXkl-3lIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning Curve"
      ],
      "metadata": {
        "id": "DBJFLrnh3lIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Combine x_train and x_test to form new dataframe for learning curve plotting\n",
        "x = pd.concat([x_train, x_test], axis=0)\n",
        "\n",
        "# Combine y_train and y_test to form new dataframe for learning curve plotting\n",
        "y = pd.concat([y_train, y_test], axis=0)\n",
        "\n",
        "# Create learning curve function\n",
        "def plot_learning_curve_classification(model, X, y, cv, scoring, title):\n",
        "    train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=cv, scoring=scoring)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Number of Training Samples\")\n",
        "    plt.ylabel(scoring)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training \" + scoring)\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Testing \" + scoring)\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.show()\n",
        "\n",
        "# plot learning curve\n",
        "plot_learning_curve_classification(logistic_regression, x, y, cv=10, scoring='accuracy', title=\"Logistic Regression Learning Curve\")\n",
        "plot_learning_curve_classification(logistic_regression, x, y, cv=10, scoring='roc_auc', title=\"Logistic Regression Learning Curve\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:38:22.428193Z",
          "iopub.execute_input": "2023-11-17T09:38:22.42856Z",
          "iopub.status.idle": "2023-11-17T09:38:23.743803Z",
          "shell.execute_reply.started": "2023-11-17T09:38:22.428526Z",
          "shell.execute_reply": "2023-11-17T09:38:23.742834Z"
        },
        "trusted": true,
        "id": "rfQYSqsl3lIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Report"
      ],
      "metadata": {
        "id": "NiYN5y3-3lIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "# Define function to calculate our classification report\n",
        "def evaluate(model, name, x_test = x_test, y_test = y_test, x_train=x_train, y_train=y_train):\n",
        "\n",
        "    y_pred_train = model.predict(x_train)\n",
        "    print('======= Data Train =======')\n",
        "    print('------')\n",
        "    print('Confusion Matrix Data Train:')\n",
        "    print()\n",
        "    print(classification_report(y_train, y_pred_train))\n",
        "    print('------')\n",
        "    matrix = pd.crosstab(y_train, y_pred_train, rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
        "    sns.heatmap(matrix, square=True, annot=True, fmt='d', cbar=False)\n",
        "    plt.xlabel('Prediction label')\n",
        "    plt.ylabel('True Label');\n",
        "    plt.title(name)\n",
        "    plt.yticks([0.5,1.5], [ 'NO', 'YES'],va='center')\n",
        "    plt.xticks([0.5,1.5], [ 'NO', 'YES'],va='center')\n",
        "    plt.show()\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_train, y_pred_train).ravel()\n",
        "    print('Assuming 1 as positive, we get for Data Train:')\n",
        "    print('')\n",
        "    print('True Positive:', tp)\n",
        "    print('True Negative:', tn)\n",
        "    print('False Positive:', fp)\n",
        "    print('False Negative:', fn)\n",
        "\n",
        "    print('')\n",
        "\n",
        "    precision = tp/(tp+fp)\n",
        "    recall = tp/(tp+fn)\n",
        "    print('Accuracy Data Train:', accuracy_score(y_train, y_pred_train))\n",
        "    print('Precision Data Train:', precision)\n",
        "    print('Recall Data Train:', recall)\n",
        "    print('F1-Score Data Train:', 2 * (precision * recall)/(precision + recall))\n",
        "\n",
        "    y_pred_test = model.predict(x_test)\n",
        "    print('======= Data Test =======')\n",
        "    print('------')\n",
        "    print('Confusion Matrix:')\n",
        "    print()\n",
        "    print(classification_report(y_test, y_pred_test))\n",
        "    print('------')\n",
        "    matrix = pd.crosstab(y_test, y_pred_test, rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
        "    sns.heatmap(matrix, square=True, annot=True, fmt='d', cbar=False)\n",
        "    plt.xlabel('Prediction label')\n",
        "    plt.ylabel('True Label');\n",
        "    plt.title(name)\n",
        "    plt.yticks([0.5,1.5], [ 'NO', 'YES'],va='center')\n",
        "    plt.xticks([0.5,1.5], [ 'NO', 'YES'],va='center')\n",
        "    plt.show()\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_test).ravel()\n",
        "    print('Assuming 1 as positive, we get:')\n",
        "    print('')\n",
        "    print('True Positive:', tp)\n",
        "    print('True Negative:', tn)\n",
        "    print('False Positive:', fp)\n",
        "    print('False Negative:', fn)\n",
        "\n",
        "    print('')\n",
        "\n",
        "    precision = tp/(tp+fp)\n",
        "    recall = tp/(tp+fn)\n",
        "    print('Accuracy Data Test:', accuracy_score(y_test, y_pred_test))\n",
        "    print('Precision Data Test:', precision)\n",
        "    print('Recall Data Test:', recall)\n",
        "    print('F1-Score Data Test:', 2 * (precision * recall)/(precision + recall))\n",
        "\n",
        "# Get our classification report\n",
        "evaluate(logistic_regression, 'Logistic Regression Evaluation')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:38:23.745129Z",
          "iopub.execute_input": "2023-11-17T09:38:23.74544Z",
          "iopub.status.idle": "2023-11-17T09:38:24.112005Z",
          "shell.execute_reply.started": "2023-11-17T09:38:23.745413Z",
          "shell.execute_reply": "2023-11-17T09:38:24.111075Z"
        },
        "trusted": true,
        "id": "DPGFAi8S3lIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. K-Nearest Neighbors"
      ],
      "metadata": {
        "id": "zx1NCEcy3lIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy"
      ],
      "metadata": {
        "id": "eUnoN-gv3lIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Find optimum n_neighbor\n",
        "error = []\n",
        "for i in range(1,31,1):\n",
        "    knn =  KNeighborsClassifier(n_neighbors=i)\n",
        "    knn.fit(x_train,y_train)\n",
        "    preds = knn.predict(x_test)\n",
        "    error.append(np.mean(preds!=y_test))\n",
        "n_neighbor = error.index(np.min(error))+1\n",
        "print(f'optimum n_neighbor: {n_neighbor}')\n",
        "\n",
        "# Apply hyperparameter tuning to find optimum other parameters\n",
        "param = {\n",
        "    'p':[1.0,1.5,2.0],\n",
        "    'leaf_size': [20,30,40],\n",
        "    'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "}\n",
        "knn = KNeighborsClassifier(n_neighbors=n_neighbor)\n",
        "bayes_search = BayesSearchCV(estimator=knn, search_spaces=param,\n",
        "                             cv=10,n_jobs=-1,n_iter=30,scoring='roc_auc')\n",
        "bayes_search.fit(x_train, y_train)\n",
        "print(\"Parameter Terbaik:\", bayes_search.best_params_)\n",
        "\n",
        "# Fit the model\n",
        "knn = KNeighborsClassifier(**bayes_search.best_params_,n_neighbors=n_neighbor)\n",
        "knn.fit(x_train, y_train)\n",
        "\n",
        "# Calculating accuracy\n",
        "print(f\"Train Accuracy : {accuracy_score(y_train, knn.predict(x_train))*100} %\" )\n",
        "print(f\"Test Accuracy : {accuracy_score(y_test, knn.predict(x_test))*100} %\" )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:38:24.113264Z",
          "iopub.execute_input": "2023-11-17T09:38:24.113555Z",
          "iopub.status.idle": "2023-11-17T09:38:57.925129Z",
          "shell.execute_reply.started": "2023-11-17T09:38:24.113529Z",
          "shell.execute_reply": "2023-11-17T09:38:57.92414Z"
        },
        "trusted": true,
        "id": "PmtPtjah3lIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning Curve"
      ],
      "metadata": {
        "id": "ezWmSl8Z3lIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot learning curve\n",
        "plot_learning_curve_classification(knn, x, y, cv=10, scoring='accuracy', title=\"KNN Learning Curve\")\n",
        "plot_learning_curve_classification(knn, x, y, cv=10, scoring='roc_auc', title=\"KNN Learning Curve\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:38:57.926406Z",
          "iopub.execute_input": "2023-11-17T09:38:57.926772Z",
          "iopub.status.idle": "2023-11-17T09:39:03.025185Z",
          "shell.execute_reply.started": "2023-11-17T09:38:57.926744Z",
          "shell.execute_reply": "2023-11-17T09:39:03.02414Z"
        },
        "trusted": true,
        "id": "iSrmrNwp3lIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Report"
      ],
      "metadata": {
        "id": "nTm2BIwf3lIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get our classification report\n",
        "evaluate(knn, 'KNN Evaluation')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:39:03.03008Z",
          "iopub.execute_input": "2023-11-17T09:39:03.030407Z",
          "iopub.status.idle": "2023-11-17T09:39:03.508474Z",
          "shell.execute_reply.started": "2023-11-17T09:39:03.03038Z",
          "shell.execute_reply": "2023-11-17T09:39:03.507528Z"
        },
        "trusted": true,
        "id": "KoWx2rhd3lIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Naive Bayes"
      ],
      "metadata": {
        "id": "9O6JPJfb3lIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy"
      ],
      "metadata": {
        "id": "K6U115lq3lIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "naive_model = GaussianNB()\n",
        "\n",
        "\n",
        "# Apply hyperparameter tuning to find optimum other parameters\n",
        "param = {'priors':[[0.03,0.97],[0.04,0.96],[0.05,0.95],[0.1,0.9]],\n",
        "    'var_smoothing':[1e-14,1e-13,1e-12,1e-11,1e-10,1e-9,1e-8]\n",
        "}\n",
        "naive_model = GaussianNB()\n",
        "grid_search = GridSearchCV(estimator=naive_model, param_grid=param,\n",
        "                             cv=10,n_jobs=-1,scoring='roc_auc')\n",
        "grid_search.fit(x_train, y_train)\n",
        "print(\"Parameter Terbaik:\", grid_search.best_params_)\n",
        "\n",
        "# Fit the model\n",
        "naive_model = GaussianNB(**grid_search.best_params_)\n",
        "naive_model.fit(x_train, y_train)\n",
        "\n",
        "# Calculating accuracy\n",
        "print(f\"Train Accuracy : {accuracy_score(y_train, naive_model.predict(x_train))*100} %\" )\n",
        "print(f\"Test Accuracy : {accuracy_score(y_test, naive_model.predict(x_test))*100} %\" )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:39:03.509751Z",
          "iopub.execute_input": "2023-11-17T09:39:03.510052Z",
          "iopub.status.idle": "2023-11-17T09:39:04.17261Z",
          "shell.execute_reply.started": "2023-11-17T09:39:03.510024Z",
          "shell.execute_reply": "2023-11-17T09:39:04.171695Z"
        },
        "trusted": true,
        "id": "6E1y7Iv43lIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning Curve"
      ],
      "metadata": {
        "id": "zEr5js2d3lId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot learning curve\n",
        "plot_learning_curve_classification(naive_model, x, y, cv=10, scoring='accuracy', title=\"Naive Model Learning Curve\")\n",
        "plot_learning_curve_classification(naive_model, x, y, cv=10, scoring='roc_auc', title=\"Naive Model Learning Curve\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:39:04.173933Z",
          "iopub.execute_input": "2023-11-17T09:39:04.174554Z",
          "iopub.status.idle": "2023-11-17T09:39:05.476428Z",
          "shell.execute_reply.started": "2023-11-17T09:39:04.174517Z",
          "shell.execute_reply": "2023-11-17T09:39:05.473983Z"
        },
        "trusted": true,
        "id": "OENOaC0g3lId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Report"
      ],
      "metadata": {
        "id": "QQdLn1X03lId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get our classification report\n",
        "evaluate(naive_model, 'Naive Model Evaluation')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:39:05.477548Z",
          "iopub.execute_input": "2023-11-17T09:39:05.477953Z",
          "iopub.status.idle": "2023-11-17T09:39:05.856942Z",
          "shell.execute_reply.started": "2023-11-17T09:39:05.477924Z",
          "shell.execute_reply": "2023-11-17T09:39:05.855963Z"
        },
        "trusted": true,
        "id": "MK7wib5S3lId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Support Vector Machine"
      ],
      "metadata": {
        "id": "Bo39dIFs3lId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy"
      ],
      "metadata": {
        "id": "gMa130wh3lIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svc = SVC(random_state = 42, class_weight = {0: df['Grade'].value_counts(normalize=True)[1],\n",
        "                                                     1: df['Grade'].value_counts(normalize=True)[0]})\n",
        "\n",
        "# Apply hyperparameter tuning\n",
        "param = {\n",
        "    'tol': [0.0001, 0.001, 0.01],\n",
        "    'kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "    'C': [0.5,1,1.5],\n",
        "    'degree':[2,3,4,5,6],\n",
        "    'coef0':[0.0,0.5,1.0,2.0]\n",
        "}\n",
        "\n",
        "bayes_search = BayesSearchCV(estimator=svc, search_spaces=param,\n",
        "                             cv=10,n_jobs=-1, n_iter=30, scoring='roc_auc')\n",
        "bayes_search.fit(x_train, y_train)\n",
        "print(\"Parameter Terbaik:\", bayes_search.best_params_)\n",
        "\n",
        "# Fit the model\n",
        "svc = SVC(**bayes_search.best_params_,random_state = 42,\n",
        "                  class_weight = {0: df['Grade'].value_counts(normalize=True)[1],\n",
        "                                  1: df['Grade'].value_counts(normalize=True)[0]})\n",
        "svc.fit(x_train, y_train)\n",
        "\n",
        "# Calculating accuracy\n",
        "print(f\"Train Accuracy : {accuracy_score(y_train, svc.predict(x_train))*100} %\" )\n",
        "print(f\"Test Accuracy : {accuracy_score(y_test, svc.predict(x_test))*100} %\" )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:39:05.858094Z",
          "iopub.execute_input": "2023-11-17T09:39:05.858379Z",
          "iopub.status.idle": "2023-11-17T09:42:06.030765Z",
          "shell.execute_reply.started": "2023-11-17T09:39:05.858353Z",
          "shell.execute_reply": "2023-11-17T09:42:06.029716Z"
        },
        "trusted": true,
        "id": "m7uC5epM3lIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning Curve"
      ],
      "metadata": {
        "id": "xTxIkUAn3lIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot learning curve\n",
        "plot_learning_curve_classification(svc, x, y, cv=10, scoring='accuracy', title=\"SVC Learning Curve\")\n",
        "plot_learning_curve_classification(svc, x, y, cv=10, scoring='roc_auc', title=\"SVC Model Learning Curve\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:42:06.031993Z",
          "iopub.execute_input": "2023-11-17T09:42:06.032524Z",
          "iopub.status.idle": "2023-11-17T09:42:08.219566Z",
          "shell.execute_reply.started": "2023-11-17T09:42:06.032498Z",
          "shell.execute_reply": "2023-11-17T09:42:08.2186Z"
        },
        "trusted": true,
        "id": "Ve2iIm8R3lIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Report"
      ],
      "metadata": {
        "id": "E3P6YuEe3lIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get our classification report\n",
        "evaluate(svc, 'SVC Model Evaluation')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-17T09:42:08.221215Z",
          "iopub.execute_input": "2023-11-17T09:42:08.221615Z",
          "iopub.status.idle": "2023-11-17T09:42:08.533731Z",
          "shell.execute_reply.started": "2023-11-17T09:42:08.221578Z",
          "shell.execute_reply": "2023-11-17T09:42:08.532797Z"
        },
        "trusted": true,
        "id": "THDTH0UL3lIf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "3369f486-6d7f-4566-86ef-cf1c9c7f80ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'evaluate' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b33d7f4f5b94>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get our classification report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SVC Model Evaluation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Comparison"
      ],
      "metadata": {
        "id": "6KJIFK-G3lIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy"
      ],
      "metadata": {
        "id": "_qXKUIkd3lIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare both training and test accuracy for all our four models.\n",
        "\n",
        "Logistic Regression:<br>\n",
        "Train Accuracy : 85.34107402031931 %<br>\n",
        "Test Accuracy : 84.97109826589595 %\n",
        "\n",
        "KNN:<br>\n",
        "Train Accuracy : 87.08272859216255 %<br>\n",
        "Test Accuracy : 87.86127167630057 %\n",
        "\n",
        "Naive Bayes:<br>\n",
        "Train Accuracy : 85.19593613933236 %<br>\n",
        "Test Accuracy : 84.97109826589595 %\n",
        "\n",
        "SVC:<br>\n",
        "Train Accuracy : 86.35703918722787 %<br>\n",
        "Test Accuracy : 85.54913294797689 %\n",
        "\n",
        "Our KNN model has the highest test accuracy by almost 0.879. This is higher than training accuracy from (Tasci et al., 2022) at 0.876 and (Noviandy et al., 2023) at 0.875. Moreover we are only using 5 features, of which only 4 features are genetic features, compared to (Tasci et al., 2022) at 14.9 features (mean) and (Noviandy et al., 2023) at 5 features."
      ],
      "metadata": {
        "id": "NND1Z0Qq3lIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "We have found KNN model as our best model which cut required features from 23 features to just 5 features, of which only 4 are molecular features. This slightly increase the reliability of machine learning modeling in solving Glioma grading problem while saving additional cost."
      ],
      "metadata": {
        "id": "wMfyBHnf3lIh"
      }
    }
  ]
}